# -*- coding: utf-8 -*-
"""NewLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zkDRNEyGu-I-iU0avBZmVYT9euY77zpd
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np

dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)
train_dataset, test_dataset = dataset['train'], dataset['test']


BUFFER_SIZE=10000
BATCH_SIZE=64
train_dataset=train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
test_dataset=test_dataset.batch(BATCH_SIZE)



for example, label in train_dataset.take(1):
    print('texts: ', example.numpy()[:3])
    print()
    print('labels: ', label.numpy()[:3])






VOCAB_SIZE=1000
encoder=tf.keras.layers.experimental.preprocessing.TextVectorization(
    max_tokens=VOCAB_SIZE)
encoder.adapt(train_dataset.map(lambda text, label: text))
#vocab=np.array(encoder.get_vocabulary())

model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=16,
        mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])



model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(0.1),
              metrics=['accuracy'])
history = model.fit (train_dataset, epochs=2,
                      validation_data=test_dataset,
                      validation_steps=30)



sample_texts = [('The movie was terrible. The animation and the graphics '
                 'were a disaster. I would never recommend this movie.'),
                ('I liked the movie very much')]



predictions = model.predict(np.array(sample_texts))
print(predictions)